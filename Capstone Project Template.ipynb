{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Immigration Data Engineering Project\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The purpose of this project is to build data lakes for the Analytics team, so that they can use it further for the data analysis tasks in  fast and efficient manner. Data used in this project come from a variety of sources. Mainly 4 datasets are used which are as follows:\n",
    "*  I94 Immigration Data\n",
    "*  World Temperature Data\n",
    "*  U.S. City Demographic Data\n",
    "*  Airport Code Data\n",
    "\n",
    "A detailed project overview can be found in Step 1.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the necessary libraries\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime,timedelta\n",
    "from pyspark.sql.functions import from_unixtime, unix_timestamp, to_date, expr,\\\n",
    "                                  date_add,udf,col,avg,mean,year,month,split\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### 1.1 Scope: \n",
    "**Why using data lakes over data warehouse?**\n",
    "> *The idea here is to use these raw data files to build refined data lakes tables. The benefit of data lakes over data ware house is that it has more flexibility in terms of data availability, storage and data management. In the data lakes provides the data in more manageable way, at the same time it does not modify the raw data too much, by having this it allows the analaytics team more flexilbility and they can mold the data as per their requirement.*  \n",
    "\n",
    "**How end solution look like?**\n",
    "> *Final product is a data lake in form of Parquet files having following tables:*\n",
    "> 1. Population Table\n",
    "    * Columns: city, state_code, state, median_age, male, female, total, age_household_size.\n",
    "> 2. Airport Table\n",
    "    * Columns: type, name, elevation_ft, continent, iso_country, iso_region, municipality, \n",
    "                  gps_code, iata_code, local_code, co_ordinates.\n",
    "> 3. Temperature Table\n",
    "    * Columns: month, city, avg_temp, avg_temp_uncertainty.\n",
    "> 4. Immigration Table\n",
    "    * Columns: i94port, i94mon, i94yr, i94addr, i94bir, count.    \n",
    "\n",
    "**What tool do I use?**\n",
    "> *I used following tools in this project:*\n",
    "  * Apache Spark\n",
    "  * Pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "Mainly 4 datasets are used which are as follows:\n",
    ">  \n",
    "   *  I94 Immigration Data\n",
    "   *  World Temperature Data\n",
    "   *  U.S. City Demographic Data\n",
    "   *  Airport Code Table\n",
    "\n",
    "Let's have a detailed look on each dataset:\n",
    "*  **I94 Immigration Data:** This data comes from the US National Tourism and Trade Office. It contains international visitor arrival statistics by world regions and selected countries, type of visa, mode of transportation, age groups, states visited (first intended address only), and the top ports of entry. Details of the columns like their names and meanings can be seen in the data dictionary included at the end of this document.\n",
    "\n",
    "*  **World Temperature Data:**  This dataset came from Kaggle. This dataset contains temperature data of US from 1850 to 2013. A detailed data dictionary is provided at the end of this documents.\n",
    "\n",
    "* **U.S. City Demographic Data:** This data comes from Opensoft. This data comes from the US Census Bureau's 2015 American Community Survey. This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. A detailed data dictionary is provided at the end of this documents.\n",
    "\n",
    "* **Airport Code Table:** This is a simple table of airport codes and corresponding cities. The airport codes may refer to either IATA airport code, a three-letter code which is used in passenger reservation, ticketing and baggage-handling systems, or the ICAO airport code which is a four letter code used by ATC systems and for airports that do not have an IATA airport code. \n",
    "A detailed data dictionary is provided at the end of this documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1 Importing Immigration dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Spark Session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version is :: 2.4.3\n"
     ]
    }
   ],
   "source": [
    "# Checking spark version for compatibility issue\n",
    "print(\"Spark Version is :: {}\".format(spark.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.5 ms, sys: 5 ms, total: 25.5 ms\n",
      "Wall time: 837 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Loading immigration dataset\n",
    "# First loading Jun'16 file (different number of columns )\n",
    "df_immig_merged =spark.read.format('com.github.saurfang.sas.spark').option(\"inferSchema\", \"true\").\\\n",
    "                 option(\"dateFormat\", \"yyyyMMdd\").\\\n",
    "                 load('../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat')\n",
    "df_immig_merged=df_immig_merged.drop('validres','delete_days','delete_mexl','delete_dup','delete_recdup','delete_visa')\n",
    "# Adding rest files of the year\n",
    "files=['i94_jan16_sub','i94_feb16_sub','i94_mar16_sub','i94_apr16_sub','i94_may16_sub','i94_jul16_sub','i94_aug16_sub',\\\n",
    "      'i94_sep16_sub','i94_oct16_sub','i94_nov16_sub','i94_dec16_sub']\n",
    "for file in files:\n",
    "    path='../../data/18-83510-I94-Data-2016/{}.{}'.format(file,'sas7bdat')\n",
    "    #print(path)\n",
    "    df_immig =spark.read.format('com.github.saurfang.sas.spark').option(\"inferSchema\", \"true\").\\\n",
    "              option(\"dateFormat\", \"yyyyMMdd\").load(path)\n",
    "    df_immig_merged = df_immig_merged.union(df_immig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cicid=4.0, i94yr=2016.0, i94mon=6.0, i94cit=135.0, i94res=135.0, i94port='XXX', arrdate=20612.0, i94mode=None, i94addr=None, depdate=None, i94bir=59.0, i94visa=2.0, count=1.0, dtadfile=None, visapost=None, occup=None, entdepa='Z', entdepd=None, entdepu='U', matflag=None, biryear=1957.0, dtaddto='10032016', gender=None, insnum=None, airline=None, admnum=14938462027.0, fltno=None, visatype='WT')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting a few records\n",
    "df_immig_merged.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2 Importing world_temperature dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.39 ms, sys: 0 ns, total: 2.39 ms\n",
      "Wall time: 442 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Importing world temperature dataset\n",
    "df_temp=spark.read.format('csv').option('header','True').\\\n",
    "load('../../data2/GlobalLandTemperaturesByCity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Ã…rhus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspecting a few records\n",
    "df_temp.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3 Importing airport_code dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.6 ms, sys: 0 ns, total: 2.6 ms\n",
      "Wall time: 351 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Importing airport code dataset\n",
    "df_port=spark.read.format('csv').option('header','True').load('airport-codes_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ident='00A', type='heliport', name='Total Rf Heliport', elevation_ft='11', continent='NA', iso_country='US', iso_region='US-PA', municipality='Bensalem', gps_code='00A', iata_code=None, local_code='00A', coordinates='-74.93360137939453, 40.07080078125')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting a few records\n",
    "df_port.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.4 Importing us_cities_demographics dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing US cities demographic dataset\n",
    "df_demog=spark.read.format('csv').option('header','True').option('delimiter',';').\\\n",
    "         load('us-cities-demographics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(City='Silver Spring', State='Maryland', Median Age='33.8', Male Population='40601', Female Population='41862', Total Population='82463', Number of Veterans='1562', Foreign-born='30908', Average Household Size='2.6', State Code='MD', Race='Hispanic or Latino', Count='25924')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting a few records\n",
    "df_demog.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Explore the Data \n",
    "*In this section of the code, different data anomalies (like missing values, duplicate data) are identified by all datasets one by one and those issues are fixed in cleaning dataset step.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.1 Exploring Immigration dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 28\n",
      "Number of Rows: 40790529\n"
     ]
    }
   ],
   "source": [
    "# Printing shape of the dataset\n",
    "print(\"Number of Columns: {}\".format(len(df_immig_merged.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_immig_merged.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing schema of the dataset\n",
    "df_immig_merged.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.2 Exploring world_temperature dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 7\n",
      "Number of Rows: 8599212\n"
     ]
    }
   ],
   "source": [
    "# Printing Shape of the Dataset\n",
    "print(\"Number of Columns: {}\".format(len(df_temp.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_temp.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing schema of the dataset\n",
    "df_temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Duplicates Values\n",
    "df_temp=df_temp.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.3 Exploring airport_code dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing Schema of the dataset\n",
    "df_port.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Duplicates Values\n",
    "df_port=df_port.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.4 Exploring us_cities_demographics dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing schema of the dataset\n",
    "df_demog.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicate values\n",
    "df_demog=df_demog.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Cleaning Steps\n",
    "In this section I am going to discuss various steps taken to clean the raw data, aggragte it and make it ready for data pileline step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 Cleaning Immigration dataset:\n",
    "*Followings are the steps listed for cleaning immigration dataset with explanatory remarks:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting data types\n",
    "df_immig_merged=df_immig_merged.withColumn(\"cicid\", df_immig_merged[\"cicid\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"i94yr\", df_immig_merged[\"i94yr\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"i94mon\", df_immig_merged[\"i94mon\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"i94cit\", df_immig_merged[\"i94cit\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"i94res\", df_immig_merged[\"i94res\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"i94mode\", df_immig_merged[\"i94mode\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"i94bir\", df_immig_merged[\"i94bir\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"i94visa\", df_immig_merged[\"i94visa\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"biryear\", df_immig_merged[\"biryear\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"admnum\", df_immig_merged[\"admnum\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"count\", df_immig_merged[\"count\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"arrdate\", df_immig_merged[\"arrdate\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"depdate\", df_immig_merged[\"depdate\"].cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing records with i94mode='2','3' and '9'\n",
    "df_immig_merged=df_immig_merged.filter('i94mode=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating data by airport by month\n",
    "df_immig_agg=df_immig_merged.groupBy('i94port','i94mon').agg({'i94yr':'first','i94addr':'first',\\\n",
    "                                        'i94bir':'avg','count':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns' names\n",
    "df_immig_agg=df_immig_agg.select(col(\"i94port\").alias(\"i94port\"),col(\"i94mon\").alias(\"i94mon\"),\n",
    "                col(\"sum(count)\").alias(\"count\"),col(\"first(i94addr)\").alias(\"i94addr\"),\\\n",
    "                col(\"avg(i94bir)\").alias(\"i94bir\"),col(\"first(i94yr)\").alias(\"i94yr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 6\n",
      "Number of Rows: 2273\n"
     ]
    }
   ],
   "source": [
    "# Printing dataset size\n",
    "print(\"Number of Columns: {}\".format(len(df_immig_agg.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_immig_agg.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary for the airport code to airport name mapping\n",
    "dict ={}\n",
    "with open('city_code.csv') as f:\n",
    "    file=csv.DictReader(f,delimiter=',')\n",
    "    for line in file:\n",
    "        dict[line['code']]=line['city_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting airport code into airport name/city\n",
    "convert_udf = udf(lambda x: dic[x])\n",
    "df_immig_agg = df_immig_agg.withColumn('i94port', convert_udf('i94port').alias('city'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+-----+-------+------------------+-----+\n",
      "|      i94port|i94mon|count|i94addr|            i94bir|i94yr|\n",
      "+-------------+------+-----+-------+------------------+-----+\n",
      "|    Champlain|     2|   89|     NY|30.103448275862068| 2016|\n",
      "|Christiansted|     3|  276|   null|              43.0| 2016|\n",
      "|    Cleveland|     5|  327|     OH|44.764525993883794| 2016|\n",
      "|       Calais|    10|    1|     ME|              61.0| 2016|\n",
      "|    Charlotte|     4|16085|     FL| 43.73229717127759| 2016|\n",
      "+-------------+------+-----+-------+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspecting a few records\n",
    "df_immig_agg.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 Cleaning world_temperature dataset:\n",
    "*Followings are the steps listed for cleaning  temperature dataset with explanatory remarks:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting data types\n",
    "df_temp=df_temp.withColumn(\"dt\", df_temp[\"dt\"].cast('date'))\n",
    "df_temp=df_temp.withColumn(\"AverageTemperature\", df_temp[\"AverageTemperature\"].cast('float'))\n",
    "df_temp=df_temp.withColumn(\"AverageTemperatureUncertainty\", df_temp[\"AverageTemperatureUncertainty\"].cast('float'))\n",
    "df_temp=df_temp.withColumn(\"Latitude\", df_temp[\"Latitude\"].cast('float'))\n",
    "df_temp=df_temp.withColumn(\"Longitude\", df_temp[\"Longitude\"].cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: date (nullable = true)\n",
      " |-- AverageTemperature: float (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: float (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: float (nullable = true)\n",
      " |-- Longitude: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing Schema\n",
    "df_temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering data of US only\n",
    "df_temp=df_temp.filter('Country=\"United States\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns' names\n",
    "df_temp=df_temp.select(col(\"dt\").alias(\"date\"),year(\"dt\").alias(\"year\"),month(\"dt\").alias(\"month\"),\\\n",
    "                col(\"AverageTemperature\").alias(\"avg_temp\"),col(\"AverageTemperatureUncertainty\").\\\n",
    "                alias(\"avg_temp_uncertainty\"),col(\"City\").alias(\"city\"),col(\"Latitude\").alias(\"latitude\"),\\\n",
    "                col(\"Longitude\").alias(\"longitude\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating data by city by month\n",
    "df_temp_agg=df_temp.groupBy('city','month').agg({'avg_temp':'avg','avg_temp_uncertainty':'avg'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns' names\n",
    "df_temp_agg=df_temp_agg.select(col(\"city\").alias(\"city\"),col(\"month\").alias(\"month\"),\n",
    "                col(\"avg(avg_temp)\").alias(\"avg_temp\"),\\\n",
    "                col(\"avg(avg_temp_uncertainty)\").alias(\"avg_temp_uncertainty\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 4\n",
      "Number of Rows: 2976\n"
     ]
    }
   ],
   "source": [
    "# Printing Shape of the Dataset\n",
    "print(\"Number of Columns: {}\".format(len(df_temp_agg.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_temp_agg.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------------------+--------------------+\n",
      "|    city|month|          avg_temp|avg_temp_uncertainty|\n",
      "+--------+-----+------------------+--------------------+\n",
      "|New York|    8| 21.30470931622409|   1.154484500401011|\n",
      "|New York|    5|14.827175792306662|    1.16700780892279|\n",
      "|New York|    4| 8.641388474060939|   1.275153846007127|\n",
      "|New York|   11|4.5656346096442295|  1.3679269235008038|\n",
      "|New York|    6|19.867753850496733|   1.256369226339918|\n",
      "+--------+-----+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspecting a few records\n",
    "df_temp_agg.filter('city=\"New York\"').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3 Cleaning airport_code dataset:\n",
    "*Followings are the steps listed for cleaning airport dataset with explanatory remarks:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing dataset's schema\n",
    "df_port.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting data types\n",
    "df_port=df_port.withColumn(\"elevation_ft\", df_port[\"elevation_ft\"].cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting airport type i.e. 'small_airport','medium_airport', 'large_airport'\n",
    "df_port=df_port.where(col(\"type\").isin({\"small_airport\", \"medium_airport\",\"large_airport\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing extraneous columns\n",
    "df_port=df_port.drop('ident')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates rows\n",
    "df_port=df_port.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting column 'iso_region' to extract state code\n",
    "df_port = df_port.withColumn('iso_region', split(df_port['iso_region'], '-').getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|         type|            name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-------------+----------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|small_airport|The Farm Airport|       375.0|       NA|         US|        GA|Wrightsville|    01GE|     null|      01GE|-82.7711029052734...|\n",
      "+-------------+----------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspecting a few records\n",
    "df_port.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 11\n",
      "Number of Rows: 39132\n"
     ]
    }
   ],
   "source": [
    "# Printing Shape of the Dataset\n",
    "print(\"Number of Columns: {}\".format(len(df_port.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_port.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Cleaning us_demographic dataset:\n",
    "*Followings are the steps listed for cleaning us_demographic dataset with explanatory remarks:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping extraneous columns\n",
    "df_demog=df_demog.drop('Number of Veterans','Foreign-born','Race','Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting data types\n",
    "df_demog=df_demog.withColumn(\"Median Age\", df_demog[\"Median Age\"].cast('float'))\n",
    "df_demog=df_demog.withColumn(\"Male Population\", df_demog[\"Male Population\"].cast('integer'))\n",
    "df_demog=df_demog.withColumn(\"Female Population\", df_demog[\"Female Population\"].cast('integer'))\n",
    "df_demog=df_demog.withColumn(\"Total Population\", df_demog[\"Total Population\"].cast('integer'))\n",
    "df_demog=df_demog.withColumn(\"Average Household Size\", df_demog[\"Average Household Size\"].cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming coulmns' names\n",
    "df_demog=df_demog.select(col(\"Median Age\").alias(\"median_age\"),col(\"Male Population\").alias(\"male\"),\\\n",
    "                col(\"Female Population\").alias(\"female\"),col(\"Total Population\").alias(\"total\"),\\\n",
    "                col(\"Average Household Size\").alias(\"avg_household_size\"),\\\n",
    "                col(\"City\").alias(\"city\"),col(\"State\").alias(\"state\"),\\\n",
    "                col(\"State Code\").alias(\"state_code\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Duplicate rows\n",
    "df_demog=df_demog.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- median_age: float (nullable = true)\n",
      " |-- male: integer (nullable = true)\n",
      " |-- female: integer (nullable = true)\n",
      " |-- total: integer (nullable = true)\n",
      " |-- avg_household_size: float (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing schema\n",
    "df_demog.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating US demographic data by city\n",
    "df_demog_agg=df_demog.groupBy('city').agg({'median_age':'first','male':'first','female':'first',\\\n",
    "            'total':'first','avg_household_size':'first','state_code':'first','state':'first'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns' names\n",
    "df_demog_agg=df_demog_agg.select(col(\"city\").alias(\"city\"),col(\"first(median_age)\").alias(\"median_age\"),\\\n",
    "                col(\"first(male)\").alias(\"male\"),\\\n",
    "                col(\"first(female)\").alias(\"female\"),col(\"first(total)\").alias(\"total\"),\\\n",
    "                col(\"first(avg_household_size)\").alias(\"avg_household_size\"),\\\n",
    "                col(\"first(state)\").alias(\"state\"),\\\n",
    "                col(\"first(state_code)\").alias(\"state_code\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+------+------+------------------+--------+----------+\n",
      "|        city|median_age| male|female| total|avg_household_size|   state|state_code|\n",
      "+------------+----------+-----+------+------+------------------+--------+----------+\n",
      "|   Brentwood|      34.2|31395| 32397| 63792|              4.98|New York|        NY|\n",
      "|New Rochelle|      40.6|38871| 40967| 79838|              2.85|New York|        NY|\n",
      "| Cheektowaga|      40.7|37476| 38599| 76075|               2.3|New York|        NY|\n",
      "|Mount Vernon|      38.5|31876| 36745| 68621|              2.85|New York|        NY|\n",
      "|     Yonkers|      38.0|96580|104538|201118|               2.8|New York|        NY|\n",
      "+------------+----------+-----+------+------+------------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing a few records for sanity check\n",
    "df_demog_agg.filter('state_code=\"NY\"').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing shape of the dataset\n",
    "print(\"Number of Columns: {}\".format(len(df_demog_agg.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_demog_agg.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "To map out the conceptual data model, I chosen star schema to design the data model. Star schema is widely used in datawarehouse data modeling. It is easy to understand, flexible and scalable. These are some prominent reason I chose star schema. \n",
    "##### 3.1.1 Data modeling using Star Schema: \n",
    "In our data modeling following are the dimension and fact tables:- \n",
    "##### Dimension Tables:\n",
    "> 1. Population Table\n",
    "    * Columns: city, state_code, state, median_age, male, female, total, age_household_size.\n",
    "> 2. Airport Table\n",
    "    * Columns: type, name, elevation_ft, continent, iso_country, iso_region, municipality, \n",
    "                  gps_code, iata_code, local_code, co_ordinates.\n",
    "> 3. Temperature Table\n",
    "    * Columns: month, city, avg_temp, avg_temp_uncertainty.\n",
    "               \n",
    "##### Fact Table:\n",
    "> 1. Immigration Table\n",
    "    * Columns: i94port, i94mon, i94yr, i94addr, i94bir, count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A diagramatic representation of the schema and its relationships (Ref: [dbdigram.io](https://dbdiagram.io/d))*\n",
    "![Image Not Found!](Schema.png \"Schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "*Followings are steps involved in data pipelines design and implementation:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.1. Implementing data pipelines for Population Table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Population Table Schema:**\n",
    "> *CREATE TABLE IF NOT EXISTS \n",
    "      population( city VARCHAR NOT NULL, \n",
    "                  state_code VARCHAR NOT NULL,\n",
    "                  state VARCHAR,\n",
    "                  median_age FLOAT, \n",
    "                  male INTEGER,\n",
    "                  female INTEGER,\n",
    "                  total INTEGER,\n",
    "                  avg_household_size FLOAT,\n",
    "                  PRIMARY KEY(state_code,city)\n",
    "                  )*\n",
    "                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- median_age: float (nullable = true)\n",
      " |-- male: integer (nullable = true)\n",
      " |-- female: integer (nullable = true)\n",
      " |-- total: integer (nullable = true)\n",
      " |-- avg_household_size: float (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing dataset schema\n",
    "df_demog_agg.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing population table data in form of parquet files and partioned by state then city.\n",
    "df_demog_agg.write.partitionBy('state','city').option('compression','snappy').parquet(\"population\",mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.2 Implementing data pipelines for Temperature Table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temperature Table Schema:**\n",
    "> *CREATE TABLE IF NOT EXISTS \n",
    "      temperature(month INTEGER NOT NULL, \n",
    "                  city VARCHAR NOT NULL,\n",
    "                  country VARCHAR NOT NULL, \n",
    "                  avg_temp FLOAT, \n",
    "                  avg_temp_uncertainty FLOAT,\n",
    "                  PRIMARY KEY (month,city))*\n",
    "                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- avg_temp: double (nullable = true)\n",
      " |-- avg_temp_uncertainty: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing dataset schema\n",
    "df_temp_agg.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing temperature table data in form of parquet files and partioned by city then month.\n",
    "df_temp_agg.write.partitionBy('city','month').option('compression','snappy').parquet(\"temperature\",mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.3. Implementing data pipelines for Airport Table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Airport Table Schema:**\n",
    "> *CREATE TABLE IF NOT EXISTS \n",
    "      airport(    type VARCHAR NOT NULL,\n",
    "                  name VARCHAR NOT NULL, \n",
    "                  elevation_ft FLOAT, \n",
    "                  continent VARCHAR,\n",
    "                  iso_country VARCHAR,\n",
    "                  iso_region VARCHAR NOT NULL,\n",
    "                  municipality VARCHAR,\n",
    "                  gps_code VARCHAR PRIMARY KEY,\n",
    "                  iata_code VARCHAR,\n",
    "                  local_code VARCHAR,\n",
    "                  co_ordinates VARCHAR,\n",
    "                  PRIMARY KEY(name, iso_region)*\n",
    "                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: float (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing dataset schema\n",
    "df_port.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing airport table data in form of parquet files and partioned by iso_region.\n",
    "df_port.write.partitionBy('iso_region').option('compression','snappy').parquet(\"airport\",mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.4. Implementing data pipelines for Immigration Table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Immigration Table Schema:**\n",
    "> *CREATE TABLE IF NOT EXISTS \n",
    "      i94( i94port STRING NOT NULL,\n",
    "           i94mon INTEGER NOT NULL,\n",
    "           i94yr INTEGER NOT NULL,\n",
    "           i94addr VARCHAR,\n",
    "           i94bir FLOAT,\n",
    "           count INTEGER,\n",
    "           PRIMARY KEY(i94port, i94mon, i94yr)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- i94mon: integer (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94yr: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing dataset schema\n",
    "df_immig_agg.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing immigration table data in form of parquet files and partioned by airport by month.\n",
    "df_immig_agg.write.partitionBy('i94port','i94mon').option('compression','snappy').parquet(\"immigration\",mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.1. Running quality checks for Population Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading population table which is in parquet file format \n",
    "df_pop=spark.read.option('compression','snappy').parquet(\"population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 8\n",
      "Number of Rows: 567\n"
     ]
    }
   ],
   "source": [
    "# Printing number of records and columns in the population table\n",
    "print(\"Number of Columns: {}\".format(len(df_pop.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_pop.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.2. Running quality checks for Temperature Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading temperature table which is in parquet file format \n",
    "df_temp=spark.read.option('compression','snappy').parquet(\"temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 4\n",
      "Number of Rows: 2976\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "print(\"Number of Columns: {}\".format(len(df_temp.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_temp.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------+-----+\n",
      "|          avg_temp|avg_temp_uncertainty|    city|month|\n",
      "+------------------+--------------------+--------+-----+\n",
      "|6.4173079751743565|  1.2440000039599695| Buffalo|    4|\n",
      "|11.944357548337994|  0.5757030320890022| Antioch|    3|\n",
      "|12.716219483352289|  0.7552743885756993| Norwalk|   11|\n",
      "|14.243999977701717|  0.8946804154241822| Killeen|    3|\n",
      "| 7.688056985948988|  0.9391036319686341|Mesquite|   12|\n",
      "+------------------+--------------------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.3. Running quality checks for Airport Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading airport table which is in parquet file format \n",
    "df_port=spark.read.option('compression','snappy').parquet(\"airport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 11\n",
      "Number of Rows: 39132\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "print(\"Number of Columns: {}\".format(len(df_port.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_port.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------------+---------+-----------+-------------------+--------+---------+----------+--------------------+----------+\n",
      "|         type|                name|elevation_ft|continent|iso_country|       municipality|gps_code|iata_code|local_code|         coordinates|iso_region|\n",
      "+-------------+--------------------+------------+---------+-----------+-------------------+--------+---------+----------+--------------------+----------+\n",
      "|small_airport|Lagrone Ranch Air...|       567.0|       NA|         US|Mc Clendon-Chisholm|    19TA|     null|      19TA|-96.4169006347656...|        TX|\n",
      "|small_airport|Temple Ranch Airport|       490.0|       NA|         US|              Freer|    42XS|     null|      42XS|-98.403889, 27.95...|        TX|\n",
      "|small_airport|Anchorage Farm Field|       440.0|       NA|         US|          Warrenton|    56TX|     null|      56TX|-96.7593994140625...|        TX|\n",
      "|small_airport|Kubecka Flying Se...|        65.0|       NA|         US|               Edna|    6TE5|     null|      6TE5|-96.6219024658203...|        TX|\n",
      "|small_airport|  6666 Ranch Airport|      1775.0|       NA|         US|            Guthrie|    6TE6|     null|      6TE6|-100.347999573, 3...|        TX|\n",
      "+-------------+--------------------+------------+---------+-----------+-------------------+--------+---------+----------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_port.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.4. Running quality checks for Immigration Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading immigration table which is in parquet file format \n",
    "df_fact=spark.read.option('compression','snappy').parquet(\"immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 6\n",
      "Number of Rows: 2273\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "print(\"Number of Columns: {}\".format(len(df_fact.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_fact.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------------------+-----+-------+------+\n",
      "|count|i94addr|            i94bir|i94yr|i94port|i94mon|\n",
      "+-----+-------+------------------+-----+-------+------+\n",
      "|   12|     TX|43.833333333333336| 2016|  other|     1|\n",
      "|   56|     TX|27.785714285714285| 2016|  other|     1|\n",
      "|    4|     PR|              54.0| 2016|  other|     5|\n",
      "|   25|   null|             39.96| 2016|  other|     5|\n",
      "|    1|     TX|              39.0| 2016|  other|    11|\n",
      "+-----+-------+------------------+-----+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "*Data dictionary is an important tool to understand the underlying data. It is data about the data. So, before start working on the data model it is essential to know the context of the data, its use case and where it came from. Hence, let's dive into the details of the data :*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.1 Immigration Table:\n",
    "*Followings are the fields of Immigration table:*\n",
    "~~~ \n",
    "  * i94yr    - This field contains year in 'YYYY' format E.g. '2016'.\n",
    "  * i94mon   - This field contains month in numeric format('1','2','3','4','5','6','7',\n",
    "               '8','9','10','11','12').\n",
    "  * i94port  - This field contains airport names(or the cities name) E.g. 'New York'. \n",
    "  * i94addr  - This field contains US state code E.g. 'NY','AL' etc.\n",
    "  * i94bir   - This field contains average age (grouped by airport and month ) of \n",
    "               Respondent in Years E.g. '44.50'.\n",
    "  * count    - This field contains summary statistics number of count (grouped by airport and month ).\n",
    " ~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2 Temperature Table:\n",
    "*Followings are the fields of temperature table:*\n",
    "```\n",
    "* Month - This field contains month in numeric format('1','2','3','4','5','6','7','8','9','10','11','12'). \n",
    "* City  - This field contains US city names E.g. 'New York','Chicago' etc.\n",
    "* AverageTemperature - This field contains average temperature (summarized by city and month) E.g. '23.44'\n",
    "* AverageTemperatureUncertainty - This field contains average temperature uncertainty \n",
    "                                  (summarized by city and month) E.g. '3.44'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.3 Population Table:\n",
    "*Followings are the fields of the population table:*\n",
    "```\n",
    "* City - This field contains US cities name E.g. 'New York', 'Chicago' etc.\n",
    "* State - Ths field contains US states name E.g. 'Arizona','California' etc.\n",
    "* State Code - This field contains US states code E.g. 'AZ', 'NY' etc.\n",
    "* Median Age - This field contains median age (by city).\n",
    "* Male       - This field contains male pupulation (by city).\n",
    "* Female     - This field contains female pupulation (by city).\n",
    "* Total      - This field contains total pupulation (by city).\n",
    "* Average Household Size - This field contains average household size (by city) E.g. '2.54'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.4 Airport Codes:\n",
    "```\n",
    "* type          - This field contains type of airport e.g. 'small_airport', \n",
    "                  'medium_airport', 'large_airport' etc.\n",
    "* name          - This field contains Airport full name.\n",
    "* elevation_ft  - This field contains Elevation of the port measured in feet.  \n",
    "* continent     - This field contains Continent code (e.g 'AS' for Asia,'NA' for North America).\n",
    "* iso_country   - This field contains two letter ISO country code assigned by \n",
    "                  International Organization for Standardization E.g. 'US' etc.\n",
    "* iso_region    - This field contains iso region code assigned by ISO E.g. 'US-AL'. \n",
    "                  Modified into US state code E.g. 'NY'.\n",
    "* municipality  - This field contains local Municipality name.  \n",
    "* gps_code      - This field contains Global Positioning System code.\n",
    "* iata_code     - This field contains IATA(Internation Air Transport Association) airport code, also known as IATA \n",
    "                  location identifier is a three letter code assigned to airports around the world.         \n",
    "* local_code    - This field contains local_code.\n",
    "* coordinates   - This field contains (x,y) co-ordinates of the location.\n",
    "```       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Complete Project Write Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Tools and Technologies:\n",
    "In this project main challenges that I see are:\n",
    "1. How to handle large amount of data in efficient and fast way.\n",
    "2. Data are coming from various heterogeneous ecosystems and that pose a greate challenge while establishing relationship between them.\n",
    "\n",
    "> *Keeping point 1 in my mind Apache Spark is the clear and straight choice for me in this case because eventhough I am more comfortable in using Pandas that won't help me here given the fact large amount of data. Apache Spark is fast and efficient in handly large amount of data. \n",
    "Secondly, After choosing Apache Spark as tool it gives option of different programming language for the coding like Scala, Java and Python.\n",
    "Again Python API Pyspark is the choice here as it has quite similarity with Pandas and easy to implements things in it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Data Updation policy:\n",
    "*As nowadays volume of fliers have been increased significantly, I reckon a daily updation of the data would be appropriate choice.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 My approach in following cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####    5.3.1 If the data was increased by 100x.\n",
    "*If the was increased by 100x then also Apache Spark would be fast and reliable choice and the above given solution would work fine.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.2 The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "*In this case, I need to use a tool like Apache Airflow to pragramatically schedule and monitor the whole process of the data pipelines.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3 The database needed to be accessed by 100+ people.\n",
    "*If the database needed to be accessed by 100+ people then I think a cloud based data warehouse(E.g. Amazon Redshift) would be be\n",
    "more effiecient data modeling option.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "1. https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "2. https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "3. https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/\n",
    "4. https://stackoverflow.com/questions/51830697/convert-date-from-integer-to-date-format\n",
    "5. https://stackoverflow.com/questions/16253060/how-to-convert-country-names-to-iso-3166-1-alpha-2-values-using-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
