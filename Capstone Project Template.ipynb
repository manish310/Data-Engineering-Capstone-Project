{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Immigration Data Engineering Project\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The purpose of this project is to build data lakes for the Analytics team, so that they can use it further for the data analysis tasks in  fast and efficient manner. Data used in this project come from a variety of sources. Mainly 4 datasets are used which are as follows:\n",
    "*  I94 Immigration Data\n",
    "*  World Temperature Data\n",
    "*  U.S. City Demographic Data\n",
    "*  Airport Code Data\n",
    "\n",
    "A detailed project overview can be found in Step 1.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime,timedelta\n",
    "from pyspark.sql.functions import from_unixtime, unix_timestamp, to_date, expr,date_add,udf,col,avg,mean,round\n",
    "from pyspark.sql.types import StringType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "Mainly 4 datasets are used which are as follows:\n",
    ">  \n",
    "   *  I94 Immigration Data\n",
    "   *  World Temperature Data\n",
    "   *  U.S. City Demographic Data\n",
    "   *  Airport Code Table\n",
    "\n",
    "Let's have a detailed look on each dataset:\n",
    "*  **I94 Immigration Data:** This data comes from the US National Tourism and Trade Office. It contains international visitor arrival statistics by world regions and selected countries, type of visa, mode of transportation, age groups, states visited (first intended address only), and the top ports of entry. Details of the columns like their names and meanings can be seen in the data dictionary included at the end of this document.\n",
    "\n",
    "*  **World Temperature Data:**  This dataset came from Kaggle. This dataset contains temperature data of US from 1850 to 2013. A detailed data dictionary is provided at the end of this documents.\n",
    "\n",
    "* **U.S. City Demographic Data:** This data comes from Opensoft. This data comes from the US Census Bureau's 2015 American Community Survey. This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. A detailed data dictionary is provided at the end of this documents.\n",
    "\n",
    "* **Airport Code Table:** This is a simple table of airport codes and corresponding cities. The airport codes may refer to either IATA airport code, a three-letter code which is used in passenger reservation, ticketing and baggage-handling systems, or the ICAO airport code which is a four letter code used by ATC systems and for airports that do not have an IATA airport code. \n",
    "A detailed data dictionary is provided at the end of this documents.\n",
    "\n",
    "**Why using data lakes over data warehouse?**\n",
    "> *The idea here is to use these raw data files to build refined data lakes tables. The benefit of data lakes over data ware house is that it has more flexibility in terms of data availability, storage and data management. In the data lakes provides the data in more manageable way, at the same time it does not modify the raw data too much, by having this it allows the analaytics team more flexilbility and they can mold the data as per their requirement.*  \n",
    "\n",
    "**How end solution look like?**\n",
    "> *So final product is a cloud based data lake having following tables:*\n",
    "\n",
    "**What tool do I use?**\n",
    "> *I used following tools in this project:*\n",
    "  * Apache Spark\n",
    "  * Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Importing Immigration dataset:\n",
    "*Immigration dataset if quite large in size so its better to use **Apache Spark** to read the files in this case.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Spark Session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version is :: 2.4.3\n"
     ]
    }
   ],
   "source": [
    "# Checking spark version for compatibility issue\n",
    "print(\"Spark Version is :: {}\".format(spark.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.6 ms, sys: 3.52 ms, total: 25.1 ms\n",
      "Wall time: 899 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Loading immigration dataset\n",
    "# First loading Jun'16 file (different number of columns )\n",
    "df_immig_merged =spark.read.format('com.github.saurfang.sas.spark').option(\"inferSchema\", \"true\").option(\"dateFormat\", \"yyyyMMdd\").\\\n",
    "    load('../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat')\n",
    "df_immig_merged=df_immig_merged.drop('validres','delete_days','delete_mexl','delete_dup','delete_recdup','delete_visa')\n",
    "# Adding rest files of the year\n",
    "files=['i94_jan16_sub','i94_feb16_sub','i94_mar16_sub','i94_apr16_sub','i94_may16_sub','i94_jul16_sub','i94_aug16_sub',\\\n",
    "      'i94_sep16_sub','i94_oct16_sub','i94_nov16_sub','i94_dec16_sub']\n",
    "for file in files:\n",
    "    path='../../data/18-83510-I94-Data-2016/{}.{}'.format(file,'sas7bdat')\n",
    "    #print(path)\n",
    "    df_immig =spark.read.format('com.github.saurfang.sas.spark').option(\"inferSchema\", \"true\").option(\"dateFormat\", \"yyyyMMdd\").\\\n",
    "    load(path)\n",
    "    df_immig_merged = df_immig_merged.union(df_immig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cicid=4.0, i94yr=2016.0, i94mon=6.0, i94cit=135.0, i94res=135.0, i94port='XXX', arrdate=20612.0, i94mode=None, i94addr=None, depdate=None, i94bir=59.0, i94visa=2.0, count=1.0, dtadfile=None, visapost=None, occup=None, entdepa='Z', entdepd=None, entdepu='U', matflag=None, biryear=1957.0, dtaddto='10032016', gender=None, insnum=None, airline=None, admnum=14938462027.0, fltno=None, visatype='WT')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting a few records\n",
    "df_immig_merged.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Importing world_temperature dataset:\n",
    "*World temperature dataset if quite large in size so its better to use **Apache Spark** to read the files in this case.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.58 ms, sys: 293 µs, total: 1.87 ms\n",
      "Wall time: 250 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_temp=spark.read.format('csv').option('header','True').\\\n",
    "load('../../data2/GlobalLandTemperaturesByCity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Importing airport_code dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.89 ms, sys: 44 µs, total: 1.93 ms\n",
      "Wall time: 250 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_port=spark.read.format('csv').option('header','True').load('airport-codes_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ident='00A', type='heliport', name='Total Rf Heliport', elevation_ft='11', continent='NA', iso_country='US', iso_region='US-PA', municipality='Bensalem', gps_code='00A', iata_code=None, local_code='00A', coordinates='-74.93360137939453, 40.07080078125')]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_port.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Importing us_cities_demographics dataset:\n",
    "*us_cities_demographics dataset is not that much large in size its better to use **Pandas** to read the files in this case.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "df_demog=spark.read.format('csv').option('header','True').option('delimiter',';').\\\n",
    "load('us-cities-demographics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|         City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|              Race|Count|\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|Hispanic or Latino|25924|\n",
      "|       Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|             White|58723|\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demog.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Exploring Immigration dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 28\n",
      "Number of Rows: 40790529\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "print(\"Number of Columns: {}\".format(len(df_immig_merged.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_immig_merged.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates value\n",
    "#df_immig=df_immig.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 3096313\n"
     ]
    }
   ],
   "source": [
    "# Number of rows after removing duplicates\n",
    "# print(\"Number of Rows: {}\".format(df_immig.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe\n",
    "#df_immig.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immig_merged.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Exploring world_temperature dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 7\n",
      "Number of Rows: 8599212\n"
     ]
    }
   ],
   "source": [
    "#Shape of the Dataset\n",
    "print(\"Number of Columns: {}\".format(len(df_temp.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_temp.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Shape of the dataset\n",
    "df_temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values\n",
    "# df_temp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Duplicates Values\n",
    "# df_port.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Exploring airport_code dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the dataset\n",
    "# df_port.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking missing values\n",
    "df_port.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Duplicates Values\n",
    "# df_port.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Exploring us_cities_demographics dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the dataset\n",
    "#df_demog.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "df_demog.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "#df_demog.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Cleaning Immigration dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning of Immigration dataset\n",
    "df_immig_merged=df_immig_merged.withColumn(\"cicid\", df_immig_merged[\"cicid\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"i94yr\", df_immig_merged[\"i94yr\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"i94mon\", df_immig_merged[\"i94mon\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"i94cit\", df_immig_merged[\"i94cit\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"i94res\", df_immig_merged[\"i94res\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"i94mode\", df_immig_merged[\"i94mode\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"i94bir\", df_immig_merged[\"i94bir\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"i94visa\", df_immig_merged[\"i94visa\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"biryear\", df_immig_merged[\"biryear\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"admnum\", df_immig_merged[\"admnum\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"count\", df_immig_merged[\"count\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"arrdate\", df_immig_merged[\"arrdate\"].cast('integer'))\n",
    "df_immig_merged=df_immig_merged.withColumn(\"depdate\", df_immig_merged[\"depdate\"].cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_immig=df_immig.select(\"arrdate\", timedelta(days=df_immig[\"arrdate\"])+datetime(1960,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_date = udf(lambda x: date_add(to_date('1960-01-01'),x))\n",
    "# df_immig = df_immig.withColumn('arrdate',convert_date(df_immig.arrdate) )\n",
    "#df_immig = df_immig.withColumn('arrdate', date_add(to_date('1960-01-01'),'arrdate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_date_udf = udf(lambda x: datetime.datetime(1960, 1, 1)+datetime.timedelta(days=x))\n",
    "# df_immig = df_immig.withColumn('arrdate', convert_date_udf('arrdate').alias('arrdate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark.sql.functions as F\n",
    "# df_dc = spark.createDataFrame([['1960-01-01']], ['report_date'])\n",
    "# df_dc.withColumn('report_date_10', F.date_add(df_dc['report_date'],20566)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_date(x):\n",
    "#     mDt = datetime(1960, 1, 1)\n",
    "#     dlt = mDt + timedelta(days=x)\n",
    "#     return dlt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# convert_date_udf = udf(lambda z: convert_date(z), StringType())\n",
    "# df_immig = df_immig.withColumn('arrdate', convert_date_udf('arrdate').alias('arrdate')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_immig.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: integer (nullable = true)\n",
      " |-- i94yr: integer (nullable = true)\n",
      " |-- i94mon: integer (nullable = true)\n",
      " |-- i94cit: integer (nullable = true)\n",
      " |-- i94res: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: integer (nullable = true)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: integer (nullable = true)\n",
      " |-- i94bir: integer (nullable = true)\n",
      " |-- i94visa: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: integer (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: integer (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immig_merged.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cleaning world_temperature dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning of Immigration dataset\n",
    "df_temp=df_temp.withColumn(\"AverageTemperature\", df_temp[\"AverageTemperature\"].cast('float'))\n",
    "df_temp=df_temp.withColumn(\"AverageTemperatureUncertainty\", df_temp[\"AverageTemperatureUncertainty\"].cast('float'))\n",
    "df_temp=df_temp.withColumn(\"Latitude\", df_temp[\"Latitude\"].cast('float'))\n",
    "df_temp=df_temp.withColumn(\"Longitude\", df_temp[\"Longitude\"].cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: float (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: float (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: float (nullable = true)\n",
      " |-- Longitude: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp=df_temp.filter('Country=\"United States\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp=df_temp.select(col(\"dt\").alias(\"date\"),col(\"AverageTemperature\").alias(\"avg_temp\"),\\\n",
    "                col(\"AverageTemperatureUncertainty\").alias(\"avg_temp_uncertainty\"),col(\"City\").alias(\"city\"),\\\n",
    "                col(\"Latitude\").alias(\"latitude\"),col(\"Longitude\").alias(\"longitude\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp.filter('dt>=\"2013-01-01\"').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 6\n",
      "Number of Rows: 687289\n"
     ]
    }
   ],
   "source": [
    "#Shape of the Dataset\n",
    "print(\"Number of Columns: {}\".format(len(df_temp.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_temp.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "# import pycountry\n",
    "# df_temp=df_temp.withColumn(\"AverageTemperature\", df_temp[\"AverageTemperature\"].cast('float'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Cleaning airport_code dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_port.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing airport which are closed now.\n",
    "df_port=df_port.filter('type!=\"closed\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Cleaning us_demographic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demog.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Columns to the Use case\n",
    "df_demog=df_demog.drop('Number of Veterans','Foreign-born','Race','Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demog.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning of Demographic dataset\n",
    "df_demog=df_demog.withColumn(\"Median Age\", df_demog[\"Median Age\"].cast('float'))\n",
    "df_demog=df_demog.withColumn(\"Male Population\", df_demog[\"Male Population\"].cast('integer'))\n",
    "df_demog=df_demog.withColumn(\"Female Population\", df_demog[\"Female Population\"].cast('integer'))\n",
    "df_demog=df_demog.withColumn(\"Total Population\", df_demog[\"Total Population\"].cast('integer'))\n",
    "df_demog=df_demog.withColumn(\"Average Household Size\", df_demog[\"Average Household Size\"].cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demog=df_demog.select(col(\"Median Age\").alias(\"median_age\"),col(\"Male Population\").alias(\"male\"),\\\n",
    "                col(\"Female Population\").alias(\"female\"),col(\"Total Population\").alias(\"total\"),\\\n",
    "                col(\"Average Household Size\").alias(\"avg_household_size\"),\\\n",
    "                col(\"City\").alias(\"city\"),col(\"State\").alias(\"state\"),\\\n",
    "                col(\"State Code\").alias(\"state_code\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping 'City' column and reducing data at State level\n",
    "df_demog=df_demog.drop('city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Duplicates\n",
    "df_demog=df_demog.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- median_age: float (nullable = true)\n",
      " |-- male: integer (nullable = true)\n",
      " |-- female: integer (nullable = true)\n",
      " |-- total: integer (nullable = true)\n",
      " |-- avg_household_size: float (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demog.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demog=df_demog.groupBy('state').agg({'median_age':'avg','male':'sum','female':'sum',\\\n",
    "                                        'total':'sum','avg_household_size':'avg','state_code':'first'})\n",
    "# df_demog=df_demog.groupBy('state').agg(mean('median_age').alias('median_age'),sum('male').alias('male'),\\\n",
    "#                                        sum('female').alias('female'),mean('avg_household_size').alias('avg_household_size'),\\\n",
    "#                                        first('state_code').alias('state_code'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_demog=df_demog.WithColumnRenamed('sum(female)','female').WithColumnRenamed('sum(male)','male')\\\n",
    "#          .WithColumnRenamed('avg(avg_household_size)','avg_household_size').WithColumnRenamed('first(state_code)','state_code')\\\n",
    "#          .WithColumnRenamed('avg(median_age)','median_age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demog=df_demog.select(col(\"avg(median_age)\").alias(\"median_age\"),col(\"sum(male)\").alias(\"male\"),\\\n",
    "                col(\"sum(female)\").alias(\"female\"),col(\"sum(total)\").alias(\"total\"),\\\n",
    "                col(\"avg(avg_household_size)\").alias(\"avg_household_size\"),\\\n",
    "                col(\"state\").alias(\"state\"),\\\n",
    "                col(\"first(state_code)\").alias(\"state_code\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+-------+-------+------------------+---------+----------+\n",
      "|        median_age|   male| female|  total|avg_household_size|    state|state_code|\n",
      "+------------------+-------+-------+-------+------------------+---------+----------+\n",
      "|30.979999542236328| 530818| 519773|1050591|             3.175|     Utah|        UT|\n",
      "|41.400001525878906| 176807| 175959| 352766| 2.690000057220459|   Hawaii|        HI|\n",
      "|35.618182095614344| 702157| 720246|1422403|2.5009090900421143|Minnesota|        MN|\n",
      "| 35.54999961853027|1177546|1256143|2433689|2.2990000009536744|     Ohio|        OH|\n",
      "|32.766666094462074| 286479| 303400| 589879|2.5299999713897705| Arkansas|        AR|\n",
      "+------------------+-------+-------+-------+------------------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demog.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Star Schema:\n",
    "##### Dimension Tables:\n",
    "> 1. Immigrant Table\n",
    "    * Columns: gender, biryear, occup, i94bir\n",
    "##### Fact Table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension Tables : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Immigrant Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Immigrant Table Schema:**\n",
    "> *CREATE TABLE IF NOT EXISTS \n",
    "      i94( cicid INTEGER PRIMARY KEY, \n",
    "           gender STRING,\n",
    "           biryear INTEGER,\n",
    "           occup STRING,\n",
    "           i94bir INTEGER,\n",
    "           visatype STRING)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n",
    "immigrant_table=df_immig_merged.select(['cicid','gender','biryear','occup','i94bir','visatype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigrant_table.write.mode('overwrite').option('compression','snappy').parquet(\"immigrant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imm=spark.read.option('compression','snappy').parquet(\"immigrant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(\"Number of Columns: {}\".format(len(df_imm.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_imm.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imm.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Airline Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I94 Table Schema:**\n",
    "> *CREATE TABLE IF NOT EXISTS \n",
    "     airline( cicid INTEGER PRIMARY KEY, \n",
    "           dtafile STRING,\n",
    "           dtaddto STRING)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n",
    "airline_table=df_immig_merged.select(['cicid','airline','fltno'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_table.write.mode('overwrite').option('compression','snappy').parquet(\"airline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airline=spark.read.option('compression','snappy').parquet(\"airline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(\"Number of Columns: {}\".format(len(df_airline.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_airline.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airline.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. i94 Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I94 Table Schema:**\n",
    "> *CREATE TABLE IF NOT EXISTS \n",
    "      i94( cicid INTEGER PRIMARY KEY, \n",
    "           i94yr INTEGER,\n",
    "           i94mon INTEGER,\n",
    "           i94res INTEGER,\n",
    "           i94bir INTEGER,\n",
    "           i94port INTEGER,\n",
    "           i94mode INTEGER,\n",
    "           i94addr STRING,\n",
    "           i94visa INTEGER,\n",
    "           dtafile STRING,\n",
    "           dtaddto STRING)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n",
    "i94_table=df_immig_merged.select(['cicid','i94yr','i94mon','i94res','i94bir','i94port','i94mode','i94addr',\\\n",
    "                           'i94visa','dtadfile','dtaddto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94_table.write.mode('overwrite').option('compression','snappy').parquet(\"i94\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i94=spark.read.option('compression','snappy').parquet(\"i94\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(\"Number of Columns: {}\".format(len(df_i94.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_i94.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i94.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Population Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Population Table Schema:**\n",
    "> *CREATE TABLE IF NOT EXISTS \n",
    "      population( median_age FLOAT, \n",
    "                  male INTEGER,\n",
    "                  female INTEGER,\n",
    "                  total INTEGER,\n",
    "                  avg_household_size FLOAT, \n",
    "                  state VARCHAR PRIMARY KEY,\n",
    "                  state_code VARCHAR NOT NULL)*\n",
    "                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male: long (nullable = true)\n",
      " |-- female: long (nullable = true)\n",
      " |-- total: long (nullable = true)\n",
      " |-- avg_household_size: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demog.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demog.write.mode('overwrite').option('compression','snappy').parquet(\"population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop=spark.read.option('compression','snappy').parquet(\"population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 7\n",
      "Number of Rows: 49\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "print(\"Number of Columns: {}\".format(len(df_pop.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_pop.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+------+-------+------------------+--------+----------+\n",
      "|       median_age|  male|female|  total|avg_household_size|   state|state_code|\n",
      "+-----------------+------+------+-------+------------------+--------+----------+\n",
      "|36.37000007629395|627951|684178|1312129|2.6549999952316283|Maryland|        MD|\n",
      "+-----------------+------+------+-------+------------------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pop.filter('state=\"Maryland\"').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Temperature Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temperature Table Schema:**\n",
    "> *CREATE TABLE IF NOT EXISTS \n",
    "      temperature(date date NOT NULL, \n",
    "                  city VARCHAR NOT NULL,\n",
    "                  country VARCHAR NOT NULL, \n",
    "                  avg_temp FLOAT, \n",
    "                  avg_temp_uncertainty FLOAT,\n",
    "                  latitude FLOAT,\n",
    "                  longitude FLOAT,\n",
    "                  PRIMARY KEY (date,city))*\n",
    "                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.write.mode('overwrite').option('compression','snappy').parquet(\"temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp=spark.read.option('compression','snappy').parquet(\"temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(\"Number of Columns: {}\".format(len(df_temp.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_temp.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Airport Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Airport Table Schema:**\n",
    "> *CREATE TABLE IF NOT EXISTS \n",
    "      airport(indent VARCHAR NOT NULL, \n",
    "                  type VARCHAR NOT NULL,\n",
    "                  name VARCHAR NOT NULL, \n",
    "                  elevation_ft FLOAT, \n",
    "                  continent VARCHAR,\n",
    "                  iso_country VARCHAR,\n",
    "                  iso_region VARCHAR,\n",
    "                  municipality VARCHAR,\n",
    "                  gps_code VARCHAR PRIMARY KEY,\n",
    "                  iata_code VARCHAR,\n",
    "                  local_code VARCHAR,\n",
    "                  co_ordinates VARCHAR)*\n",
    "                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_port.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_port.write.mode('overwrite').option('compression','snappy').parquet(\"airport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_port=spark.read.option('compression','snappy').parquet(\"airport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(\"Number of Columns: {}\".format(len(df_port.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_port.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_port.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fact Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Immigration Table Schema:**\n",
    "> *CREATE TABLE IF NOT EXISTS \n",
    "      i94( cicid INTEGER PRIMARY KEY, \n",
    "           i94port STRING REFERENCES airport(iata_code),\n",
    "           i94addr STRING REFERENCES population(state_code),\n",
    "           dtafile STRING REFERENCES temperature(date),\n",
    "           dtaddto STRING,\n",
    "           arrdate INTEGER,\n",
    "           depdate INTEGER,\n",
    "           count INTEGER)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_immig.createOrReplaceTempView(\"immig\")\n",
    "# df_airline.createOrReplaceTempView(\"airline\")\n",
    "# df_i94.createOrReplaceTempView(\"i94\")\n",
    "# df_port.createOrReplaceTempView(\"port\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# immigration_table = spark.sql(\"\"\"SELECT i.cicid,i.gender,i.visatype,i.dtaddto,b.i94port,b.i94mode,p.name FROM immig  i\n",
    "#                                  LEFT OUTER JOIN i94 b ON b.cicid=i.cicid\n",
    "#                                  LEFT OUTER JOIN airline a ON i.cicid=a.cicid\n",
    "#                                  LEFT OUTER JOIN port p ON b.i94port=p.iata_code\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immig_merged.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n",
    "fact_table=df_immig_merged.select(['cicid','i94port','i94addr','dtadfile','dtaddto','arrdate','depdate','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_table.write.mode('overwrite').option('compression','snappy').parquet(\"immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact=spark.read.option('compression','snappy').parquet(\"immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 8\n",
      "Number of Rows: 3096313\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "print(\"Number of Columns: {}\".format(len(df_fact.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_fact.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+--------+--------+-------+-------+-----+\n",
      "|  cicid|i94port|i94addr|dtadfile| dtaddto|arrdate|depdate|count|\n",
      "+-------+-------+-------+--------+--------+-------+-------+-----+\n",
      "|5748517|    LOS|     CA|20160430|10292016|  20574|  20582|    1|\n",
      "|5748518|    LOS|     NV|20160430|10292016|  20574|  20591|    1|\n",
      "|5748519|    LOS|     WA|20160430|10292016|  20574|  20582|    1|\n",
      "|5748520|    LOS|     WA|20160430|10292016|  20574|  20588|    1|\n",
      "|5748521|    LOS|     WA|20160430|10292016|  20574|  20588|    1|\n",
      "+-------+-------+-------+--------+--------+-------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "_Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immigration Data:\n",
    "~~~\n",
    "  * cicid    -  \n",
    "  * i94yr    - 4 digit year\n",
    "  * i94mon   - Numeric month\n",
    "  * i94cit   - This format shows all the valid and invalid codes for processing\n",
    "  * i94res   - This format shows all the valid and invalid codes for processing\n",
    "  * i94port  - This format shows all the valid and invalid codes for processing\n",
    "  * arrdate  - is the Arrival Date in the USA. It is a SAS date numeric field that a \n",
    "               permament format has not been applied.  Please apply whichever date format \n",
    "               works for you.\n",
    "  * i94mode  - There are missing values as well as not reported (9)\n",
    "               \t1 = 'Air'\n",
    "                2 = 'Sea'\n",
    "                3 = 'Land'\n",
    "                9 = 'Not reported' ;   \n",
    "  * i94addr  - There is lots of invalid codes in this variable and the list below \n",
    "               shows what we have found to be valid, everything else goes into 'other'\n",
    "  * depdate  - is the Departure Date from the USA. It is a SAS date numeric field that \n",
    "               a permament format has not been applied.  Please apply whichever date format \n",
    "               works for you.\n",
    "  * i94bir   - Age of Respondent in Years\n",
    "  * i94visa  - Visa codes collapsed into three categories:\n",
    "               1. 1 = Business\n",
    "               2. 2 = Pleasure\n",
    "               3. 3 = Student\n",
    "  * count    - Used for summary statistics\n",
    "  * dtadfile - Character Date Field - Date added to I-94 Files\n",
    "  * visapost - Department of State where where Visa was issued\n",
    "  * occup    - Occupation that will be performed in U.S.\n",
    "  * entdepa  - Arrival Flag - admitted or paroled into the U.S.\n",
    "  * entdepd  - Departure Flag - Departed, lost I-94 or is deceased\n",
    "  * entdepu  - Update Flag - Either apprehended, overstayed, adjusted to perm residence\n",
    "  * matflag  - Match flag - Match of arrival and departure records\n",
    "  * biryear  - 4 digit year of birth\n",
    "  * dtaddto  - Character Date Field - Date to which admitted to U.S. (allowed to stay until)\n",
    "  * gender   - Non-immigrant sex\n",
    "  * insnum   - INS number\n",
    "  * airline  - Airline used to arrive in U.S.\n",
    "  * admnum   - Admission Number\n",
    "  * fltno    - Flight number of Airline used to arrive in U.S.\n",
    "  * visatype - Class of admission legally admitting the non-immigrant to temporarily stay in U.S.\n",
    "  \n",
    " ~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### World Temperature Data:\n",
    "```\n",
    "* dt\n",
    "* AverageTemperature\n",
    "* AverageTemperatureUncertainty\n",
    "* City\n",
    "* Country\n",
    "* Latitude\n",
    "* Longitude\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### U.S. City Demographic Data:\n",
    "```\n",
    "* City\n",
    "* State\n",
    "* Median Age\n",
    "* Male Population\n",
    "* Female Population\n",
    "* Total Population\n",
    "* Number of Veterans\n",
    "* Foreign-born\n",
    "* Average Household Size\n",
    "* State Code\n",
    "* Race\n",
    "* Count\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airport Codes:\n",
    "```\n",
    "* ident         - \n",
    "* type          - Type of airport e.g. 'small_airport', 'medium_airport', 'large_airport' etc\n",
    "* name          - Airport full name.\n",
    "* elevation_ft  - Elevation of the port measured in feet.  \n",
    "* continent     - Continet code (e.g 'AS' for Asia,'NA' for North America)\n",
    "* iso_country   - Two letter ISO country code assigned by International Organization for Standardization\n",
    "* iso_region    - iso region code assigned by ISO.\n",
    "* municipality  - Locality Municipality.  \n",
    "* gps_code      - Global Positioning System code.\n",
    "* iata_code     - IATA(Internation Air Transport Association) airport code, also known as IATA \n",
    "                  location identifier is a three letter code assigned to airports around the world.         \n",
    "* local_code    - Similar to gps_code.\n",
    "* coordinates   - (x,y) co-ordinates of the location.\n",
    "```       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "1. https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "2. https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "3. https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/\n",
    "4. https://stackoverflow.com/questions/51830697/convert-date-from-integer-to-date-format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
